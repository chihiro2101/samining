The software architecture used in this repository is the Anderson Acceleration (AA) technique, which is an extrapolation technique that accelerates fixed-point iterations, such as those arising from the iterative training of deep learning models. The repository implements a moving average to reduce stochastic oscillations in the fixed-point iteration, resulting in a smoother sequence of gradient descent updates that enables the use of AA. The repository also uses a criterion to automatically decide if the moving average is needed by monitoring the relative standard deviation between consecutive stochastic gradient updates. The software architecture includes the implementation of the Anderson acceleration process with specific hyperparameters such as relaxation, wait_iterations, history_depth, store_each_nth, frequency, reg_acc, safeguard, and average.